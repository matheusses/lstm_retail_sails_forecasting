# -*- coding: utf-8 -*-
"""LSTM_TE_DLF_Exercicio04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rT3IcykXGQ545OGZf36013ofavsziXfj

Reference: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
"""

import numpy as np 
import pandas as pd

from matplotlib import pylab as plt
import matplotlib.dates as mdates
# plt.rcParams['figure.figsize'] = (15.0, 8.0)
from sklearn.preprocessing import MinMaxScaler
import numpy
import matplotlib.pyplot as plt
import pandas
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM,Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

"""❖ Passo 1: carregar os dados 

Utilizar o conjunto de dados mock_kaggle.csv em anexo. Cada coluna representa data, venda, estoque e preço, respectivamente. A ideia é estimar a coluna 3 (estoque) do arquivo
"""

df = pd.read_csv("mock_kaggle.csv",header=None) 
df.head()
#Data

df.describe()

#get date and stock
df_estoque = df.iloc[:, [0, 2]]
df_estoque.head()

plt.figure(figsize=(10,4))
plt.title('Stock Per Day')
plt.xlabel('Date')
plt.ylabel('Stock in units')
plt.plot(df_estoque.iloc[:,0].values,df_estoque.iloc[:,1].values);

"""❖ Passo 2: treinar o algoritmo LSTM 

Com os dados carregados, treine o algoritmo LSTM. É importante ter atenção ao formato utilizado quando os dados são carregados. Para o treinamento utilize 916 dias e o restante, 21 dias ou 3 semanas, para teste.
"""

# fix random seed for reproducibility
np.random.seed(7)

#normalize dataset
normalizador = MinMaxScaler(feature_range=(0,1))
df_stock_normalizada = normalizador.fit_transform(df_estoque.iloc[:,[1]].values)

#split dataset
#data train
count_val_days = 22
df_stock_train = df_stock_normalizada[:len(df_estoque)-count_val_days]
#data validation
df_stock_val = df_stock_normalizada[len(df_estoque)-count_val_days:len(df_estoque)]

len(df_estoque)

len(df_stock_train)

len(df_stock_val)

df_stock_train[:5]

df_stock_val

# convert an array of values into a dataset matrix
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back):
		a = dataset[i:(i+look_back), 0]		
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)

def create_dataset(dataset, look_back=1):
  # X time series(previous values)
  dataX= []
  for i in range(0,len(dataset)-look_back):
    x = dataset[i]		
    dataX.append(x)
  # Y time series (future values, forward pass(look_back X))
  dataY =[]
  for i in range(look_back,len(dataset)):
    y = dataset[i]		
    dataY.append(y)
  return np.array(dataX), np.array(dataY)

# reshape into X=t and Y=t+1
#look back means 'x' elements before.
look_back = 1
trainX, trainY = create_dataset(df_stock_train,look_back)
testX, testY = create_dataset(df_stock_val,look_back)

len(trainX)

len(trainY)

trainX[900:]

trainY[900:]

# -1 the last value
df_stock_train[-1]

trainX[:6]

trainY[:5]

len(testX)

len(testY)

testX

testY

df_stock_val[-1]

# reshape input to be [samples, time steps, features]
trainX = np.reshape(trainX, (-1, 1, trainX.shape[1]))
testX = np.reshape(testX, (-1, 1, testX.shape[1]))

# create and fit the LSTM network
model = Sequential()
#four neurons or hidden layers
model.add(LSTM(50, input_shape=(1, look_back)))

# model.add(LSTM(50,return_sequences = True, input_shape=(1, look_back)))

# model.add(Dropout(0.3))

# model.add(LSTM(units = 50))

model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)

# make predictions
trainPredict_3 = model.predict(trainX)
testPredict_3 = model.predict(testX)
# invert predictions
trainPredict_inverse_3 = normalizador.inverse_transform(trainPredict_3)
trainY_inverse_3 = normalizador.inverse_transform(trainY)
testPredict_inverse_3 = normalizador.inverse_transform(testPredict_3)
testY_inverse_3 = normalizador.inverse_transform(testY)
# calculate root mean squared error
trainScore_3 = math.sqrt(mean_squared_error(trainY_inverse_3, trainPredict_inverse_3))
print('Train Score 3 weeks: %.2f RMSE' % (trainScore_3))
testScore_3 = math.sqrt(mean_squared_error(testY_inverse_3, testPredict_inverse_3))
print('Test Score 3 weeks: %.2f RMSE' % (testScore_3))

# Create plots to compare results between the predict and real values
def PlotResultForecast(dataset,predict,nr_week):
  fig, ax = plt.subplots()
  ax.plot(dataset, 'blue', label='Real')
  ax.plot(predict, 'red', label='Forecast')
  plt.xlabel('Days')
  plt.ylabel('Stock')
  plt.title('Shift test predictions for plotting to '+ str(nr_week) + ' weeks')
  legend = ax.legend(loc='upper center', shadow=True, fontsize='x-large')
  plt.show()

# shift test predictions for plotting to 3 weeks
PlotResultForecast(testY_inverse_3,testPredict_inverse_3,3)

#2 start weeks

# make predictions
testPredict_2 = model.predict(testX[:14])
# invert predictions
testPredict_inverse_2 = normalizador.inverse_transform(testPredict_2)
testY_inverse_2 = normalizador.inverse_transform(testY[:14])
# calculate root mean squared error
testScore_2 = math.sqrt(mean_squared_error(testY_inverse_2, testPredict_inverse_2))
print('Test Score 2 weeks: %.2f RMSE' % (testScore_2))

PlotResultForecast(testY_inverse_2,testPredict_inverse_2,2)

#1 start week

# make predictions
testPredict_1 = model.predict(testX[:7])
# invert predictions
testPredict_inverse_1 = normalizador.inverse_transform(testPredict_1)
testY_inverse_1 = normalizador.inverse_transform(testY[:7])
# calculate root mean squared error
testScore_1 = math.sqrt(mean_squared_error(testY_inverse_1, testPredict_inverse_1))
print('Test Score 1 weeks: %.2f RMSE' % (testScore_1))

# shift test predictions for plotting to 1 week
PlotResultForecast(testY_inverse_1,testPredict_inverse_1,1)

